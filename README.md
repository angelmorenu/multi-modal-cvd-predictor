# ğŸ©º Multi-Modal Predictors for Cardiovascular Disease Risk and Outcomes

**Author:** Angel Morenu  
**Email:** angel.morenu@ufl.edu  
**Affiliation:** University of Florida, M.S. in Applied Data Science  
**Course:** EEE 6778 â€“ Applied Machine Learning II (Fall 2025)  
**Instructor:** Dr. Ramirez-Salgado

---

## ğŸ§  Project Overview

Cardiovascular disease (CVD) remains the leading global cause of death. This project develops a multi-modal machine learning system that fuses:

- Tabular demographic data  
- Hospital admission records  
- Physiological 12â€‘lead ECG signals

The goal is improved CVD risk prediction with explainability and edge-deployable inference.

---

## ğŸ“¦ GitHub Repository

Repository: https://github.com/angelmorenu/multi-modal-cvd-predictor

Clone and navigate:
```bash
git clone https://github.com/angelmorenu/multi-modal-cvd-predictor.git
cd multi-modal-cvd-predictor
```

---

## ğŸ“Š Datasets Used

| Dataset | Description | Link |
|---|---:|---|
| Cardiovascular Diseases | Demographics and lifestyle features | https://www.kaggle.com/datasets/mexwell/cardiovascular-diseases |
| Hospital Admissions | Clinical visit and diagnostic records | https://www.kaggle.com/datasets/ashishsahani/hospital-admissions-data |
| PTB-XL ECG | 12-lead ECG signals and annotations | https://www.kaggle.com/datasets/khyeh0719/ptb-xl-dataset-reformatted |

---

## ğŸ—ï¸ Project Architecture

This hybrid workflow uses:
- scikit-learn for preprocessing and tabular baselines  
- PyTorch for ECG deep learning and feature fusion  
- Streamlit for the user interface  
- Conceptual Edge AI deployment (e.g., smartwatch scenario)

![Architecture Diagram](docs/multimodal_cvd_architecture.png)

---

## âš™ï¸ Installation and Environment Setup

Install with conda (choose the appropriate platform file):

```bash
# macOS (Intel or Apple Silicon M1/M2/M3; uses CPU/MPS)
conda env create -f environment.macos.yml
conda activate cvd_predictor

# Linux/Windows with NVIDIA GPU (CUDA 11.8)
conda env create -f environment.cuda.yml
conda activate cvd_predictor
```

Notes:
- On Apple Silicon, PyTorch uses the MPS backend when available.
- The default environment.yml is cross-platform (CPU/MPS-friendly).
- If conda dependency resolution is slow, install mamba:
```bash
conda install -c conda-forge mamba
```

---

## ğŸš€ Running the Project

1. Run the setup and EDA notebook:
```bash
jupyter notebook notebooks/setup.ipynb
```

2. Launch the Streamlit UI:
```bash
# Activate the conda environment
source /opt/anaconda3/etc/profile.d/conda.sh
conda activate cvd_predictor

# Run the Streamlit app
streamlit run ui/MultiModalCVD_app.py --server.port 8502 --server.headless true
```
This opens a local UI to input demographics, upload an ECG, and view predicted risk.

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8502
  Network URL: http://192.168.1.72:8502
  External URL: http://104.4.123.52:8502

---

## ğŸ“ Repository Structure

```
multi_modal_cvd_project/
â”œâ”€â”€ data/                     
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ setup.ipynb           
â”‚   â””â”€â”€ train_eval.ipynb      
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py         
â”‚   â”œâ”€â”€ model.py              
â”‚   â””â”€â”€ train.py, eval.py     
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ app.py                
â”‚   â””â”€â”€ MultiModalCVD_app.py  
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ plot_confusion.py   
|   â””â”€â”€ ui_demo.png  
â”‚ 
â”œâ”€â”€ figures
|   â””â”€â”€ confusion_matrix.png 
|   â””â”€â”€ ui_demo.png
â”‚   â””â”€â”€ multimodal_cvd_architecture.png
â”œâ”€â”€ results/                  
â”œâ”€â”€ environment.yml           
â”œâ”€â”€ environment.cuda.yml      
â”œâ”€â”€ environment.macos.yml     
â”œâ”€â”€ README.md                 
â”œâ”€â”€ Morenu_Project Deliverable 1.docx
â””â”€â”€ Morenu_Deliverable2_IEEE_Report.pdf
```

---

## ğŸ“¦ Deliverables

- Complete project repository (code + documentation)  
- Jupyter notebooks for setup and evaluation (setup.ipynb, train_eval.ipynb)  
- Streamlit application (ui/app.py and MultiModalCVD_app.py)  
- Technical IEEE report (Deliverable 2, PDF)  
- Environment YAML files for reproducibility

---

## âœ… Reproducibility Instructions (Deliverable 3 snapshot)

The repository contains scripts that regenerate the evaluation artifacts and figures used in Deliverable 3. After activating the `cvd_predictor` conda environment, follow these steps to reproduce the evaluation and the figures included in the report.

1) Re-generate predictions and the metric summary (writes per-run numpy arrays and a JSON summary):

```bash
python scripts/generate_predictions.py --outdir results/
```

This writes files like `results/tabular_y_true.npy`, `results/tabular_y_pred.npy`, `results/tabular_y_prob.npy`, and a summary JSON at `results/metric_summary.json`.

2) Recreate the main figures used in the report and UI (example commands):

```bash
# confusion matrix for fusion predictions
python scripts/plot_confusion.py --pred results/fusion_y_pred.npy --true results/fusion_y_true.npy --out figures/confusion_matrix.png

# calibration and probability histograms for the fusion model
python scripts/plot_calibration.py --probs results/fusion_y_prob.npy --true results/fusion_y_true.npy --out figures/calibration_curve_fusion.png
python scripts/perf_dashboard.py --probs results/fusion_y_prob.npy --true results/fusion_y_true.npy --out figures/perf_dashboard_fusion.png
```

3) Re-run interpretability scripts (optional â€” install `shap` and `captum` to get full outputs):

```bash
python scripts/interpretability_shap.py
python scripts/interpretability_ecg_saliency.py
```

4) Launch the Streamlit UI (interactive testing and demo):

```bash
streamlit run ui/MultiModalCVD_app.py --server.port 8502
```

Results snapshot (current `results/metric_summary.json` included in the repo):

- Tabular model: accuracy 0.5714, ROC AUC 0.5273, PR AUC 0.5596, Brier 0.3506
- ECG model: accuracy 0.4375, ROC AUC 0.2969, PR AUC 0.3755, Brier 0.3311
- Fusion (fallback/stable evaluation): accuracy 0.5625, ROC AUC 0.4531, PR AUC 0.5258, Brier 0.3617

These values are used in `Report/Project Deliverables 3.tex`.

---

## Commit hygiene (recommended)

Before committing notebooks or pushing branches, strip outputs so diffs remain clean. Two options:

- Install and use nbstripout (recommended):

```bash
# install once in your environment
pip install nbstripout
# enable the git filter for this repo
nbstripout --install
# strip outputs from all tracked notebooks
git ls-files "*.ipynb" -z | xargs -0 nbstripout
```

- Or add `.gitattributes` to enforce stripping on commit (this repo includes a starter `.gitattributes`).

Keeping notebooks output-free makes PR reviews much easier.

## Interpretability (SHAP & ECG saliency)

This repository includes lightweight interpretability scripts that produce visual summaries in `figures/`:

- `scripts/interpretability_shap.py` â€” attempts to compute SHAP values. If `shap` is not installed or available in your environment, the script falls back to plotting RandomForest feature importances (MDI). The output is written to `figures/shap_force_example.png`.
- `scripts/interpretability_ecg_saliency.py` â€” produces an ECG saliency visualization. If `captum` is available it can be integrated; otherwise the script produces a derivative-based saliency plot. The output is written to `figures/ecg_saliency.png`.

To get full SHAP/Captum outputs in your local environment, install the packages and re-run the scripts:

```bash
# Activate your conda environment first (example)
conda activate cvd_predictor

# Install interpretability libraries (may take a minute)
pip install -r requirements-interpretability.txt

# Run the interpretability scripts
python scripts/interpretability_shap.py
python scripts/interpretability_ecg_saliency.py
```

After running, confirm `figures/shap_force_example.png` and `figures/ecg_saliency.png` are created and then re-generate the PDF report.

Alternatively, install just the interpretability extras with pip:

```bash
pip install -r requirements-interpretability.txt
```

---

## ğŸ¤– Responsible AI Goals

- Fairness: Evaluate across age, gender, and race subgroups.  
- Transparency: Incorporate SHAP and saliency-based explanations.  
- Efficiency: Support lightweight edge deployment for on-device inference.  
- Reproducibility: Publish code, environment files, and metrics.

---

## ğŸ‘¤ Author & Contact

Angel Morenu  
University of Florida â€“ M.S. in Applied Data Science  
angel.morenu@ufl.edu  
GitHub: https://github.com/angelmorenu/multi-modal-cvd-predictor

This repository accompanies Deliverable 2 of EEE 6778 â€“ Applied Machine Learning II (Fall 2025).

% Project Deliverables 3 - IEEEtran two-column report
\documentclass[conference]{IEEEtran}

% --- PACKAGES ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}       % For professional tables (\toprule, \midrule, \bottomrule)
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\usepackage{verbatim}       % For the \begin{verbatim} environment
\usepackage[hidelinks]{hyperref} % For clickable links
% --- GRAPHICS PATH ---
% Include both local and parent 'figures' directories so images resolve when
% compiling from the Report/ directory or from the repository root.
\graphicspath{{../figures/}{figures/}}

% --- BEGIN DOCUMENT ---

\begin{document}

% --- TITLE ---
% Using the more standard IEEE title format from your previous version
\title{Multi-Modal Predictors for Cardiovascular Disease Risk and Outcomes: \\
Deliverable 3 -- Refinement, Usability, and Evaluation}

% --- AUTHOR ---
% The \author block must come BEFORE \maketitle
\author{
    \IEEEauthorblockN{Angel Morenu}
    \IEEEauthorblockA{
        University of Florida\\
        EEE 6778 -- Applied Machine Learning II (Fall 2025)\\
        Instructor: Dr. Ramirez-Salgado\\
        Email: angel.morenu@ufl.edu
    }
}

% --- Renders the title ---
\maketitle

% --- ABSTRACT ---
% The abstract must come AFTER \maketitle
\begin{abstract}
This deliverable documents the refinement, usability, and evaluation work performed after the initial prototype (Deliverable 2). The focus was on stabilizing preprocessing, improving model and evaluation robustness, enhancing the user interface for clarity and traceability, and producing a repeatable evaluation workflow with interpretable outputs. This report presents updated quantitative results, visual diagnostics, and a short reflection on responsible AI considerations. All artifacts, scripts, and figures referenced here are available in the repository under \path{results/} and \path{figures/}.
\end{abstract}

% --- BODY CONTENT ---

\section{Project Summary}
This work advances the multi-modal cardiovascular disease (CVD) prediction prototype from a proof-of-concept to a more robust and reproducible pipeline. The refinements target three goals: (1) stable end-to-end execution across local environments, (2) clearer and more useful interpretability and UI features for non-technical reviewers, and (3) a repeatable evaluation harness that produces metrics and visuals used in this report. Key outcomes are updated evaluation metrics (tabular, ECG-only, and fusion), a Streamlit-based interface with logging and explanation panels, and scripts to regenerate the metrics and figures used in the report.

\section{Updated System Architecture and Pipeline}
Figure~\ref{fig:evals} summarizes the evaluation and reporting outputs produced by the refined pipeline. Architecturally, the system retains the three main stages from Deliverable 2: (a) preprocessing and feature extraction, (b) model training and inference, and (c) evaluation and UI presentation. Refinements are listed below.

\subsection{Data preprocessing}
I standardized input handling by enforcing canonical shapes for ECG signals (fixed-length padding/truncation), validating tabular feature alignment with labels, and emitting deterministic warnings and truncation when splits are mismatched. This reduces runtime errors and enables reproducible small-sample experiments.

\subsection{Modeling and training}
The evaluation harness trains lightweight unimodal baselines and a fusion fallback to ensure results are always available even when the primary PyTorch checkpoint cannot be loaded. The pipeline uses scikit-learn baselines for tabular models and summary-feature RandomForest models for ECG-derived features \cite{pedregosa2011scikit}. When a compatible checkpoint is present, the fusion model uses the PyTorch-based fusion architecture described in \path{src/model.py} and the saved checkpoint at \path{artifacts/model.pt} \cite{paszke2019pytorch}.

\subsection{Evaluation and UI}
Evaluation artifacts are produced by \path{scripts/generate_predictions.py}, which writes per-run numpy arrays (for example, \path{results/tabular_y_true.npy}, \path{results/tabular_y_pred.npy}, and \path{results/tabular_y_prob.npy} with equivalent files produced for ECG and fusion runs) and a JSON summary at \path{results/metric_summary.json}. Visualization scripts (\path{scripts/plot_calibration.py}, \path{scripts/perf_dashboard.py}, \path{scripts/plot_confusion.py}) create report-ready PNGs placed in the repository-level \path{figures/} directory. The UI (\path{ui/MultiModalCVD_app.py}) was extended to show predicted probabilities, an "Explanations" expander, and persistent JSONL logging for auditability.

\section{Refinements Made Since Deliverable 2}
Below I highlight concrete improvements and why they matter.

\subsection{Cleaner and defensive preprocessing}
Previously mismatched array lengths frequently caused crashes. I now validate split lengths at load time and truncate to a safe common length with explicit warnings. ECG signals are cast or padded to a canonical length (2000 samples) ensuring consistent model input shapes. This reduces brittle runtime failures and makes small-scale experiments reproducible.

\subsection{Fallback baselines and checkpoint robustness}
To avoid failing evaluations when a saved fusion checkpoint is incompatible, the generator falls back to a stacked logistic regression over unimodal predictions. This guarantees that \path{results/metric_summary.json} is always produced and that comparison tables can be generated automatically.

\subsection{Interface and usability improvements}
The Streamlit UI now includes:
\begin{itemize}
    \item A compact input panel and \texttt{Predict} action that shows risk probability and recommended actions (color-coded).
    \item An "Explanations" expander that displays \path{figures/shap_force_example.png} and \path{figures/ecg_saliency.png} when available; scripts provide robust fallbacks when heavy libraries are not installed.
    \item JSONL logging of each prediction into \path{results/predictions_log.jsonl} (timestamp, inputs, outputs, and explanation paths) for reproducibility and audit.
\end{itemize}

\subsection{Interpretability}
The repository includes scripts for SHAP-style explanations (Lundberg \& Lee) and ECG saliency visualizations \cite{lundberg2017unified, kokhlikyan2020captum}. When \path{shap} or \path{captum} are not available, the scripts fall back to training a RandomForest to produce feature importances and computing simple derivative-based saliency for ECGs. This pragmatic approach ensures visual explanations for reviewers without heavy dependency requirements.

\section{Interface Usability and Improvements}
I performed lightweight usability checks by running the UI locally and validating the explanation flow. Screenshots (in \path{figures/ui_demo.png}) demonstrate the updated layout: inputs on the left, results and explanations on the right. Key usability outcomes:
\begin{itemize}
    \item Reduced cognitive load: results are presented as a clear probability with traffic-light coloring and short textual guidance.
    \item Traceability: each prediction is logged; reviewers can re-run analysis using the logged inputs.
    \item Robust fallbacks: if heavy explanation libraries are missing, the UI still shows fallback images and a message explaining how to enable full interpretability.
\end{itemize}

\section{Extended Evaluation and Updated Results}
I re-ran the evaluation pipeline and generated per-run predictions and summary metrics. Table~\ref{tab:comparison_multimodal} summarizes the current (Deliverable 3) metrics across unimodal and fusion runs. All values come from \path{results/metric_summary.json} generated by \path{scripts/generate_predictions.py} and are reproducible with the commands in the README.

\begin{table}[!t]
\centering
\caption{Comparison of held-out evaluation metrics for unimodal (Tabular, ECG) and fused (Fusion) models â€” Deliverable 3 snapshot}
\label{tab:comparison_multimodal}
\begin{tabular}{@{}lccc@{}}
    % --- FIXED: Changed 'oprule' to '\toprule' ---
    \toprule 
Metric & Tabular & ECG & Fusion \\
\midrule
Accuracy & 0.571 & 0.438 & 0.563 \\
ROC AUC & 0.527 & 0.297 & 0.453 \\
PR AUC & 0.560 & 0.375 & 0.526 \\
Brier Score & 0.351 & 0.331 & 0.362 \\
F1 Score & 0.609 & 0.526 & 0.588 \\
Sensitivity & 0.700 & 0.625 & 0.625 \\
Specificity & 0.455 & 0.250 & 0.500 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visual diagnostics}
Figure~\ref{fig:evals} contains the key diagnostic plots: confusion matrix, compact performance dashboard (ROC/PR/CM/prob-hist), calibration curve, and probability histogram. These plots help assess calibration, discrimination, and class-level errors.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{confusion_matrix.png}
    \includegraphics[width=0.48\textwidth]{perf_dashboard_fusion.png}\\[6pt]
    \includegraphics[width=0.48\textwidth]{calibration_curve_fusion.png}
    \includegraphics[width=0.48\textwidth]{prob_hist_fusion.png}
    \caption{Evaluation visuals produced by the refined pipeline. Top-left: confusion matrix. Top-right: performance dashboard (ROC/PR/CM/prob histogram). Bottom-left: calibration curve for the fusion model. Bottom-right: predicted probability histogram.}
    \label{fig:evals}
\end{figure*}

\subsection{Interpretability examples}
Figure~\ref{fig:interpret} shows the interpretability outputs used in the UI: a SHAP-style feature importance / force plot (or RandomForest fallback) and an ECG saliency map. These help explain which tabular features and ECG regions most influence predictions in individual examples.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\columnwidth]{shap_force_example.png}
    \includegraphics[width=0.48\columnwidth]{ecg_saliency.png}
    \caption{Interpretability examples (SHAP or RandomForest fallback on the left; ECG saliency on the right).}
    \label{fig:interpret}
\end{figure}

\subsection{Interpretation of results}
The fusion pipeline's ROC AUC (0.453) and PR AUC (0.526) indicate modest discrimination on the held-out set. The tabular model performs slightly better on average for ROC AUC in this small sample; however, the fusion model shows more balanced sensitivity/specificity. Given the small dataset and fallback baselines used here, these results illustrate pipeline stability rather than state-of-the-art performance. The focus is reproducible evaluation, accessible explanations, and a pathway for iterative improvement (e.g., more data, calibration, hyperparameter search).

\section{Responsible AI Reflection}
I considered fairness, privacy, and transparency during refinement:
\begin{itemize}
    \item Fairness: The dataset is small and demographic subgroup analyses are incomplete. Next steps: compute per-group ROC/PR and calibration, and add reweighting/oversampling if imbalances are identified.
    \item Privacy: The UI logs inputs and outputs to \path{results/predictions_log.jsonl} for reproducibility; in production this would be encrypted and consented. For the course prototype store only synthetic/demo inputs or de-identified entries.
    \item Transparency: Interpretability scripts and UI panels provide explanations; I document fallback behavior so reviewers understand when full SHAP/Captum explanations are available versus when the lightweight fallbacks are used.
\end{itemize}

\section{Repository and Artifacts (what to submit)}
The repository includes the artifacts requested for Deliverable 3:
\begin{itemize}
    \item Updated code and scripts: \path{scripts/generate_predictions.py}, \path{scripts/plot_calibration.py}, \path{scripts/perf_dashboard.py}, \path{scripts/interpretability_shap.py}, \path{scripts/interpretability_ecg_saliency.py}, \path{scripts/build_report.sh}.
    \item UI: \path{ui/MultiModalCVD_app.py} (Streamlit) with explanation expander and JSONL logging.
    \item Figures: \path{figures/*.png} including \path{confusion_matrix.png}, \path{perf_dashboard_fusion.png}, \path{calibration_curve_fusion.png}, \path{prob_hist_fusion.png}, \path{shap_force_example.png}, \path{ecg_saliency.png}, and \path{ui_demo.png}.
    \item Results: \path{results/metric_summary.json}, per-run numpy arrays such as \path{results/tabular_y_true.npy}, \path{results/tabular_y_pred.npy}, \path{results/tabular_y_prob.npy}, and \path{results/predictions_log.jsonl}.
    \item Docs: \path{README.md} updated with Deliverable 3 reproduction steps, and \path{requirements-interpretability.txt} for optional extras.
\end{itemize}

\section{How to reproduce (short)}
Clone the repository, activate the conda environment, and run:
\begin{verbatim}
python3 scripts/generate_predictions.py --outdir results/
python3 scripts/plot_calibration.py --run fusion
python3 scripts/perf_dashboard.py --run fusion
./scripts/build_report.sh  # requires pdflatex installed locally
\end{verbatim}

\section{Conclusions and Next Steps}
Deliverable 3 demonstrates a more stable, reproducible, and explainable evaluation workflow. The primary contribution is process stability: robust preprocessing, reproducible metrics and figure generation, and a UI with traceable predictions and explanations. Immediate next steps are:
\begin{enumerate}
    \item Run a systematic hyperparameter search and cross-validation to improve generalization.
    \item Perform subgroup fairness and calibration analyses (temperature scaling) and update the UI with calibration warnings.
    \item Replace fallback explainability outputs with full SHAP/Captum outputs in environments where those libraries are available.
\end{enumerate}

\section*{Acknowledgments}
Thanks to Dr. Ramirez-Salgado for providing guidance and feedback. I am grateful to the opensource community for providing tools and datasets that enabled this work.

% --- BIBLIOGRAPHY ---
% \bibliographystyle{IEEEtran} must come *before* the bibliography
\bibliographystyle{IEEEtran}
\begin{thebibliography}{9}
\bibitem{pedregosa2011scikit} F. Pedregosa \emph{et~al.}, "Scikit-learn: Machine Learning in Python," \emph{Journal of Machine Learning Research}, vol. 12, pp. 2825--2830, 2011.

\bibitem{paszke2019pytorch} A. Paszke \emph{et~al.}, "PyTorch: An Imperative Style, High-Performance Deep Learning Library," in \emph{NeurIPS}, 2019.

\bibitem{lundberg2017unified} S. M. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions," in \emph{NeurIPS}, 2017.

\bibitem{kokhlikyan2020captum} N. Kokhlikyan \emph{et~al.}, "Captum: A Model Interpretability Library for PyTorch," arXiv:2009.07896, 2020.

\bibitem{molnar2019interpretable} C. Molnar, \emph{Interpretable Machine Learning}, 2019.

% --- FIXED: Added this missing \bibitem from the text at the end ---
\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville, \emph{Deep Learning}. MIT Press, 2016.

\end{thebibliography}

% --- FIXED: Removed all the extra text that was after this line ---
\end{document}
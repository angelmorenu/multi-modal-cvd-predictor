\documentclass[conference]{IEEEtran}

% --- PACKAGES ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\graphicspath{{figures/}}

\begin{document}

	itle{Multi-Modal Predictors for Cardiovascular Disease Risk and Outcomes: \\
Deliverable 3 -- Refinement, Usability, and Evaluation}

\author{\IEEEauthorblockN{Angel Morenu}
        \IEEEauthorblockA{University of Florida\\
        EEE 6778 -- Applied Machine Learning II (Fall 2025)\\
        Email: angel.morenu@ufl.edu}
}

\maketitle

\begin{abstract}
This deliverable documents refinements made after the initial prototype (Deliverable 2). The work focused on stabilizing preprocessing, improving evaluation robustness, enhancing the user interface for clarity and traceability, and providing repeatable evaluation artifacts and interpretability outputs. We report updated quantitative results, diagnostic visuals, and a short reflection on responsible AI considerations. All scripts and figures referenced are included in the repository.
\end{abstract}

\section{Project Summary}
We advanced the multi-modal cardiovascular disease (CVD) prediction prototype by hardening preprocessing, adding robust fallback baselines, improving the user interface, and producing an automated evaluation harness. The primary goals were reproducibility, explainability, and a stable demonstration pipeline suitable for reviewers.

\section{Updated System Architecture and Pipeline}
The system retains three main components: preprocessing, modeling/inference, and evaluation/UI. Key refinements since Deliverable 2:
\begin{itemize}
    \item Defensive preprocessing: canonical ECG lengths (padding/truncation), tabular/label alignment checks and truncation, deterministic warnings when shapes mismatch.
    \item Evaluation harness: `scripts/generate_predictions.py` produces per-run results and a `results/metric_summary.json` with tabular, ECG, and fusion metrics.
    \item Visualization and reporting: plotting scripts create `figures/` assets used in the report and UI (`plot_calibration.py`, `perf_dashboard.py`, `plot_confusion.py`).
    \item Interface: `ui/MultiModalCVD_app.py` exposes predictions, an Explanations panel, and persistent JSONL logging of inputs/outputs.
\end{itemize}

\section{Refinements Made Since Deliverable 2}
We implemented several practical changes to improve stability and usability:
\begin{itemize}
    \item Input validation and truncation to a safe common length for mismatched arrays (prevents runtime failures and ensures deterministic behavior across runs).
    \item Lightweight unimodal baselines (logistic regression for tabular, RandomForest on ECG summary features) and a stacked logistic fusion fallback when the main PyTorch checkpoint is missing or incompatible.
    \item Interpretability fallbacks: SHAP attempted when available, otherwise RandomForest-based feature importance; ECG saliency computed via derivatives if gradient-based libs are absent.
    \item UI improvements: clearer layout, color-coded probability guidance, an Explanations expander that displays explanation figures, and JSONL logging to `results/predictions_log.jsonl`.
\end{itemize}

\section{Interface Usability and Improvements}
We ran local smoke tests and produced screenshots (in `figures/ui_demo.png`). Compared to Deliverable 2, the interface now:
\begin{itemize}
    \item Presents a focused prediction panel with a single probability output and concise human-readable guidance.
    \item Shows explanation thumbnails next to predictions and supplies instructions for enabling full SHAP/Captum outputs.
    \item Persists every prediction event to a JSONL audit log for traceability and reproducibility.
\end{itemize}

\section{Extended Evaluation and Updated Results}
We executed the evaluation harness to produce reproducible metrics and visuals. The scripts write per-run numpy arrays under `results/` and a single `results/metric_summary.json` that the report references. Table~\ref{tab:comparison_multimodal} shows the held-out metrics for Tabular, ECG, and Fusion models (Deliverable 3 snapshot).

\begin{table}[!t]
\centering
\caption{Held-out evaluation metrics (Deliverable 3 snapshot)}
\label{tab:comparison_multimodal}
\begin{tabular}{@{}lccc@{}}
	oprule
Metric & Tabular & ECG & Fusion \\
\midrule
Accuracy & 0.571 & 0.438 & 0.563 \\
ROC AUC & 0.527 & 0.297 & 0.453 \\
PR AUC & 0.560 & 0.375 & 0.526 \\
Brier Score & 0.351 & 0.331 & 0.362 \\
F1 Score & 0.609 & 0.526 & 0.588 \\
Sensitivity & 0.700 & 0.625 & 0.625 \\
Specificity & 0.455 & 0.250 & 0.500 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visual diagnostics}
Figure~\ref{fig:evals} contains the core diagnostic plots used to assess model calibration, discrimination, and class-level errors.

\begin{figure*}[!t]
        \centering
        \includegraphics[width=0.48\textwidth]{figures/confusion_matrix.png}
        \includegraphics[width=0.48\textwidth]{figures/perf_dashboard_fusion.png}\\[6pt]
        \includegraphics[width=0.48\textwidth]{figures/calibration_curve_fusion.png}
        \includegraphics[width=0.48\textwidth]{figures/prob_hist_fusion.png}
        \caption{Evaluation visuals produced by the refined pipeline. Top-left: confusion matrix. Top-right: performance dashboard (ROC/PR/CM/prob histogram). Bottom-left: calibration curve for the fusion model. Bottom-right: predicted probability histogram.}
        \label{fig:evals}
\end{figure*}

\subsection{Interpretability examples}
Figure~\ref{fig:interpret} shows explanation outputs used in the UI: a SHAP-style feature importance (or RandomForest fallback) and an ECG saliency map. These are generated by `scripts/interpretability_shap.py` and `scripts/interpretability_ecg_saliency.py`.

\begin{figure}[!t]
        \centering
        \includegraphics[width=0.48\columnwidth]{figures/shap_force_example.png}
        \includegraphics[width=0.48\columnwidth]{figures/ecg_saliency.png}
        \caption{Interpretability examples (SHAP or RandomForest fallback on the left; ECG saliency on the right).}
        \label{fig:interpret}
\end{figure}

\subsection{Interpretation and limitations}
The fusion pipeline shows modest discrimination (ROC AUC = 0.453) and reasonable calibration diagnostics given the small held-out set. These results primarily validate the evaluation pipeline and the UI's ability to present interpretable evidence; improving absolute performance will require larger, better-balanced data and systematic hyperparameter tuning.

\section{Responsible AI Reflection}
We documented responsible-AI considerations and implemented light safeguards:
\begin{itemize}
    \item Fairness: we plan subgroup evaluation (age, sex, race) and will apply reweighting or calibration per-group if disparities are detected.
    \item Privacy: prediction logs are stored locally and intended for debugging; in a production setting logs must be encrypted and consented.
    \item Transparency: interpretability is surfaced in the UI and documented; fallback behavior is explicit in the README.
\end{itemize}

\section{Repository artifacts and reproduction}
Included in the repository are:
\begin{itemize}
    \item Scripts: `scripts/generate_predictions.py`, `scripts/plot_calibration.py`, `scripts/perf_dashboard.py`, `scripts/plot_confusion.py`, `scripts/interpretability_shap.py`, `scripts/interpretability_ecg_saliency.py`, and `scripts/build_report.sh`.
    \item UI: `ui/MultiModalCVD_app.py` (Streamlit) with explanation expander and JSONL logging.
    \item Figures: `figures/*.png` used in this report.
    \item Results: `results/metric_summary.json` and per-run numpy arrays.
    \item Docs: updated `README.md` and `requirements-interpretability.txt`.
\end{itemize}

\section{How to reproduce (short)}
After cloning and activating the conda environment, run:
\begin{verbatim}
python3 scripts/generate_predictions.py --outdir results/
python3 scripts/plot_calibration.py --run fusion
python3 scripts/perf_dashboard.py --run fusion
./scripts/build_report.sh  # requires pdflatex installed locally
\end{verbatim}

\section{Conclusions and next steps}
Deliverable 3 delivers a stable, reproducible evaluation pipeline and an improved interface with interpretability features. Next steps are cross-validated hyperparameter optimization, subgroup fairness analysis, calibration improvements, and replacing fallbacks with full SHAP/Captum explanations when available.

\section*{Acknowledgments}
Thanks to Dr. Ramirez-Salgado and course staff for feedback.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{9}
\bibitem{pedregosa2011scikit} F. Pedregosa \emph{et~al.}, "Scikit-learn: Machine Learning in Python," \emph{Journal of Machine Learning Research}, vol. 12, pp. 2825--2830, 2011.

\bibitem{paszke2019pytorch} A. Paszke \emph{et~al.}, "PyTorch: An Imperative Style, High-Performance Deep Learning Library," in \emph{NeurIPS}, 2019.

\bibitem{lundberg2017unified} S. M. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions," in \emph{NeurIPS}, 2017.

\bibitem{kokhlikyan2020captum} N. Kokhlikyan \emph{et~al.}, "Captum: A Model Interpretability Library for PyTorch," arXiv:2009.07896, 2020.

\bibitem{molnar2019interpretable} C. Molnar, \emph{Interpretable Machine Learning}, 2019.
\end{thebibliography}

\end{document}
\documentclass[conference]{IEEEtran}

% --- PACKAGES ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\graphicspath{{figures/}}

\begin{document}

\title{Multi-Modal Predictors for Cardiovascular Disease Risk and Outcomes:\\
Deliverable 3 -- Refinement, Usability, and Evaluation}

\author{
\begin{abstract}
This deliverable documents the refinement, usability, and evaluation work performed after the initial prototype (Deliverable 2). The focus was on stabilizing preprocessing, improving model and evaluation robustness, enhancing the user interface for clarity and traceability, and producing a repeatable evaluation workflow with interpretable outputs. We present updated quantitative results, visual diagnostics, and a short reflection on responsible AI considerations. All artifacts, scripts, and figures referenced here are available in the repository under `results/` and `figures/`.
\end{abstract}

\section{Project Summary}
This work advances the multi-modal cardiovascular disease (CVD) prediction prototype from a proof-of-concept to a more robust and reproducible pipeline. The refinements target three goals: (1) stable end-to-end execution across local environments, (2) clearer and more useful interpretability and UI features for non-technical reviewers, and (3) a repeatable evaluation harness that produces metrics and visuals used in this report. Key outcomes are updated evaluation metrics (tabular, ECG-only, and fusion), a Streamlit-based interface with logging and explanation panels, and scripts to regenerate the metrics and figures used in the report.

\section{Updated System Architecture and Pipeline}
Figure~\ref{fig:evals} summarizes the evaluation and reporting outputs produced by the refined pipeline. Architecturally, the system retains the three main stages from Deliverable 2: (a) preprocessing and feature extraction, (b) model training and inference, and (c) evaluation and UI presentation. Refinements are listed below.

\subsection{Data preprocessing}
We standardized input handling by enforcing canonical shapes for ECG signals (fixed-length padding/truncation), validating tabular feature alignment with labels, and emitting deterministic warnings and truncation when splits are mismatched. This reduces runtime errors and enables reproducible small-sample experiments.

\subsection{Modeling and training}
The evaluation harness trains lightweight unimodal baselines and a fusion fallback to ensure results are always available even when the primary PyTorch checkpoint cannot be loaded. The pipeline uses scikit-learn baselines for tabular models and summary-feature RandomForest models for ECG-derived features \cite{pedregosa2011scikit}. When a compatible checkpoint is present, the fusion model uses the PyTorch-based fusion architecture described in `src/model.py` and saved in `artifacts/model.pt` \cite{paszke2019pytorch}.

\subsection{Evaluation and UI}
Evaluation artifacts are produced by `scripts/generate_predictions.py`, which writes per-run numpy arrays (`results/{tabular,ecg,fusion}_y_{true,pred,prob}.npy`) and a JSON summary (`results/metric_summary.json`). Visualization scripts (`plot_calibration.py`, `perf_dashboard.py`, `plot_confusion.py`) create report-ready PNGs placed under `figures/`. The UI (`ui/MultiModalCVD_app.py`) was extended to show predicted probabilities, an `Explanations` expander, and persistent JSONL logging for auditability.

\section{Refinements Made Since Deliverable 2}
Below we highlight concrete improvements and why they matter.

\subsection{Cleaner and defensive preprocessing}
Previously mismatched array lengths frequently caused crashes. We now validate split lengths at load time and truncate to a safe common length with explicit warnings. ECG signals are cast or padded to a canonical length (2000 samples) ensuring consistent model input shapes. This reduces brittle runtime failures and makes small-scale experiments reproducible.

\subsection{Fallback baselines and checkpoint robustness}
To avoid failing evaluations when a saved fusion checkpoint is incompatible, the generator falls back to a stacked logistic regression over unimodal predictions. This guarantees that `results/metric_summary.json` is always produced and that comparison tables can be generated automatically.

\subsection{Interface and usability improvements}
The Streamlit UI now includes:
\begin{itemize}
    \item A compact input panel and `Predict` action that shows risk probability and recommended actions (color-coded).
    \item An `Explanations` expander that displays `figures/shap_force_example.png` and `figures/ecg_saliency.png` when available; scripts provide robust fallbacks when heavy libraries are not installed.
    \item JSONL logging of each prediction into `results/predictions_log.jsonl` (timestamp, inputs, outputs, and explanation paths) for reproducibility and audit.
\end{itemize}

\subsection{Interpretability}
The repository includes scripts for SHAP-style explanations (Lundberg \& Lee) and ECG saliency visualizations \cite{lundberg2017unified, kokhlikyan2020captum}. When `shap` or `captum` are not available, the scripts fall back to training a RandomForest to produce feature importances and computing simple derivative-based saliency for ECGs. This pragmatic approach ensures visual explanations for reviewers without heavy dependency requirements.

\section{Interface Usability and Improvements}
We performed lightweight usability checks by running the UI locally and validating the explanation flow. Screenshots (in `figures/ui_demo.png`) demonstrate the updated layout: inputs on the left, results and explanations on the right. Key usability outcomes:
\begin{itemize}
    \item Reduced cognitive load: results are presented as a clear probability with traffic-light coloring and short textual guidance.
    \item Traceability: each prediction is logged; reviewers can re-run analysis using the logged inputs.
    \item Robust fallbacks: if heavy explanation libraries are missing, the UI still shows fallback images and a message explaining how to enable full interpretability.
\end{itemize}

\section{Extended Evaluation and Updated Results}
We re-ran the evaluation pipeline and generated per-run predictions and summary metrics. Table~\ref{tab:comparison_multimodal} summarizes the current (Deliverable 3) metrics across unimodal and fusion runs. All values come from `results/metric_summary.json` generated by `scripts/generate_predictions.py` and are reproducible with the commands in the README.

\begin{table}[!t]
\centering
\caption{Comparison of held-out evaluation metrics for unimodal (Tabular, ECG) and fused (Fusion) models — Deliverable 3 snapshot}
\label{tab:comparison_multimodal}
\begin{tabular}{@{}lccc@{}}
	oprule
Metric & Tabular & ECG & Fusion \\
\midrule
Accuracy & 0.571 & 0.438 & 0.563 \\
ROC AUC & 0.527 & 0.297 & 0.453 \\
PR AUC & 0.560 & 0.375 & 0.526 \\
Brier Score & 0.351 & 0.331 & 0.362 \\
F1 Score & 0.609 & 0.526 & 0.588 \\
Sensitivity & 0.700 & 0.625 & 0.625 \\
Specificity & 0.455 & 0.250 & 0.500 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visual diagnostics}
Figure~\ref{fig:evals} contains the key diagnostic plots: confusion matrix, compact performance dashboard (ROC/PR/CM/prob-hist), calibration curve, and probability histogram. These plots help assess calibration, discrimination, and class-level errors.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/confusion_matrix.png}
    \includegraphics[width=0.48\textwidth]{figures/perf_dashboard_fusion.png}\\[6pt]
    \includegraphics[width=0.48\textwidth]{figures/calibration_curve_fusion.png}
    \includegraphics[width=0.48\textwidth]{figures/prob_hist_fusion.png}
    \caption{Evaluation visuals produced by the refined pipeline. Top-left: confusion matrix. Top-right: performance dashboard (ROC/PR/CM/prob histogram). Bottom-left: calibration curve for the fusion model. Bottom-right: predicted probability histogram.}
    \label{fig:evals}
\end{figure*}

\subsection{Interpretability examples}
Figure~\ref{fig:interpret} shows the interpretability outputs used in the UI: a SHAP-style feature importance / force plot (or RandomForest fallback) and an ECG saliency map. These help explain which tabular features and ECG regions most influence predictions in individual examples.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\columnwidth]{figures/shap_force_example.png}
    \includegraphics[width=0.48\columnwidth]{figures/ecg_saliency.png}
    \caption{Interpretability examples (SHAP or RandomForest fallback on the left; ECG saliency on the right).}
    \label{fig:interpret}
\end{figure}

\subsection{Interpretation of results}
The fusion pipeline's ROC AUC (0.453) and PR AUC (0.526) indicate modest discrimination on the held-out set. The tabular model performs slightly better on average for ROC AUC in this small sample; however, the fusion model shows more balanced sensitivity/specificity. Given the small dataset and fallback baselines used here, these results illustrate pipeline stability rather than state-of-the-art performance. The focus is reproducible evaluation, accessible explanations, and a pathway for iterative improvement (e.g., more data, calibration, hyperparameter search).

\section{Responsible AI Reflection}
We considered fairness, privacy, and transparency during refinement:
\begin{itemize}
    \item Fairness: The dataset is small and demographic subgroup analyses are incomplete. Next steps: compute per-group ROC/PR and calibration, and add reweighting/oversampling if imbalances are identified.
    \item Privacy: The UI logs inputs and outputs to `results/predictions_log.jsonl` for reproducibility; in production this would be encrypted and consented. For the course prototype we only store synthetic/demo inputs or de-identified entries.
    \item Transparency: Interpretability scripts and UI panels provide explanations; we document fallback behavior so reviewers understand when full SHAP/Captum explanations are available versus when the lightweight fallbacks are used.
\end{itemize}

\section{Repository and Artifacts (what to submit)}
The repository includes the artifacts requested for Deliverable 3:
\begin{itemize}
    \item Updated code and scripts: `scripts/generate_predictions.py`, `scripts/plot_calibration.py`, `scripts/perf_dashboard.py`, `scripts/interpretability_shap.py`, `scripts/interpretability_ecg_saliency.py`, `scripts/build_report.sh`.
    \item UI: `ui/MultiModalCVD_app.py` (Streamlit) with explanation expander and JSONL logging.
    \item Figures: `figures/*.png` including `confusion_matrix.png`, `perf_dashboard_fusion.png`, `calibration_curve_fusion.png`, `prob_hist_fusion.png`, `shap_force_example.png`, `ecg_saliency.png`, and `ui_demo.png`.
    \item Results: `results/metric_summary.json`, `results/{tabular,ecg,fusion}_y_{true,pred,prob}.npy`, `results/predictions_log.jsonl`.
    \item Docs: `README.md` updated with Deliverable 3 reproduction steps, and `requirements-interpretability.txt` for optional extras.
\end{itemize}

\section{How to reproduce (short)}
Clone the repository, activate the conda environment, and run:
\begin{verbatim}
python3 scripts/generate_predictions.py --outdir results/
python3 scripts/plot_calibration.py --run fusion
python3 scripts/perf_dashboard.py --run fusion
./scripts/build_report.sh  # requires pdflatex installed locally
\end{verbatim}

\section{Conclusions and Next Steps}
Deliverable 3 demonstrates a more stable, reproducible, and explainable evaluation workflow. The primary contribution is process stability: robust preprocessing, reproducible metrics and figure generation, and a UI with traceable predictions and explanations. Immediate next steps are:
\begin{enumerate}
    \item Run a systematic hyperparameter search and cross-validation to improve generalization.
    \item Perform subgroup fairness and calibration analyses (temperature scaling) and update the UI with calibration warnings.
    \item Replace fallback explainability outputs with full SHAP/Captum outputs in environments where those libraries are available.
\end{enumerate}

\section*{Acknowledgments}
Thanks to Dr. Ramirez-Salgado and course staff for guidance and feedback.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{9}
\bibitem{pedregosa2011scikit} F. Pedregosa \emph{et~al.}, "Scikit-learn: Machine Learning in Python," \emph{Journal of Machine Learning Research}, vol. 12, pp. 2825--2830, 2011.

\bibitem{paszke2019pytorch} A. Paszke \emph{et~al.}, "PyTorch: An Imperative Style, High-Performance Deep Learning Library," in \emph{NeurIPS}, 2019.

\bibitem{lundberg2017unified} S. M. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions," in \emph{NeurIPS}, 2017.

\bibitem{kokhlikyan2020captum} N. Kokhlikyan \emph{et~al.}, "Captum: A Model Interpretability Library for PyTorch," arXiv:2009.07896, 2020.

\bibitem{molnar2019interpretable} C. Molnar, \emph{Interpretable Machine Learning}, 2019.
\end{thebibliography}

\end{document}
Specificity & 0.455 & 0.250 & 0.500 \\
\bottomrule
\end{tabular}
\end{table}

Enhancements in Deliverable~3 yield noticeably more stable predictions across modalities. Although the small dataset limits overall performance, the refined pipeline produces consistent, interpretable metrics that are reproducible across runs.

I additionally generated confusion matrices, calibration curves, and probability histograms (Fig.~\ref{fig:evals}) to better diagnose model reliability.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{confusion_matrix.png}
    \includegraphics[width=0.45\textwidth]{perf_dashboard_fusion.png}\\[4pt]
    \includegraphics[width=0.45\textwidth]{calibration_curve_fusion.png}
    \includegraphics[width=0.45\textwidth]{prob_hist_fusion.png}
    \caption{Updated evaluation visuals: confusion matrix (top-left), full performance dashboard (top-right), calibration curve (bottom-left), and predicted probability histogram (bottom-right).}
    \label{fig:evals}
\end{figure*}

% ------------------------ SECTION VI ------------------------
\section{Responsible AI Reflection}

As the system matured, I observed several areas where responsible AI considerations became increasingly relevant:

\begin{itemize}
    \item \textbf{Transparency.}  
    I improved interpretability by adding SHAP-style visualizations and ECG saliency maps, helping users understand feature contributions and waveform regions influencing predictions.

    \item \textbf{Accountability.}  
    Prediction logging now supports auditable model behavior. Each interaction is saved with inputs and outputs for later review or debugging.

    \item \textbf{Reliability and fairness.}  
    Deliverable~3 evaluation revealed calibration issues and sensitivity–specificity imbalances. I plan to apply temperature scaling, isotonic regression, and subgroup testing in the next phase.

    \item \textbf{Privacy.}  
    All processing remains strictly local. No user data leaves the device or browser session.

    \item \textbf{Sustainability.}  
    Lightweight models and efficient scripts help keep compute usage low, aligning with Green AI practices.
\end{itemize}

% ------------------------ SECTION VII ------------------------
\section{Conclusions}

Deliverable~3 demonstrates significant progress in robustness, interpretability, reproducibility, and usability. The pipeline now includes automated preprocessing alignment, standardized evaluation, interpretability visualization, extended diagnostics, and a more informative Streamlit interface. These enhancements position the system for a strong final deliverable and a compelling end-to-end demonstration of multimodal medical AI.

\section*{Acknowledgments}
I thank Dr.~Ramirez-Salgado for continued guidance and technical feedback.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{1}
\bibitem{pedregosa2011scikit}
F.~Pedregosa \emph{et al.}, ``Scikit-learn: Machine Learning in Python,'' \emph{Journal of Machine Learning Research}, vol.~12, pp.~2825--2830, 2011.

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville, \emph{Deep Learning}. MIT Press, 2016.

\end{thebibliography}

\end{document}
